{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGpl_w2Lt169"
      },
      "source": [
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import math\n",
        "import numpy as np\n",
        "#import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "# first neural network with keras tutorial\n",
        "import pandas as pd\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical \n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from keras.models import Model\n",
        "import timeit\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
        "from keras.optimizers import SGD\n",
        "#import cv2, numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z5eeo_8uGFD",
        "outputId": "9f5d0f59-5628-44e0-cfb5-4c7dea755c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "measurements = pd.read_csv('/content/drive/My Drive/kaggle_emotion.csv')\n",
        "measurements.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># mean_0_a</th>\n",
              "      <th>mean_1_a</th>\n",
              "      <th>mean_2_a</th>\n",
              "      <th>mean_3_a</th>\n",
              "      <th>mean_4_a</th>\n",
              "      <th>mean_d_0_a</th>\n",
              "      <th>mean_d_1_a</th>\n",
              "      <th>mean_d_2_a</th>\n",
              "      <th>mean_d_3_a</th>\n",
              "      <th>mean_d_4_a</th>\n",
              "      <th>mean_d_0_a2</th>\n",
              "      <th>mean_d_1_a2</th>\n",
              "      <th>mean_d_2_a2</th>\n",
              "      <th>mean_d_3_a2</th>\n",
              "      <th>mean_d_4_a2</th>\n",
              "      <th>mean_d_5_a</th>\n",
              "      <th>mean_d_6_a</th>\n",
              "      <th>mean_d_7_a</th>\n",
              "      <th>mean_d_8_a</th>\n",
              "      <th>mean_d_9_a</th>\n",
              "      <th>mean_d_10_a</th>\n",
              "      <th>mean_d_11_a</th>\n",
              "      <th>mean_d_12_a</th>\n",
              "      <th>mean_d_13_a</th>\n",
              "      <th>mean_d_14_a</th>\n",
              "      <th>mean_d_15_a</th>\n",
              "      <th>mean_d_16_a</th>\n",
              "      <th>mean_d_17_a</th>\n",
              "      <th>mean_d_18_a</th>\n",
              "      <th>mean_d_19_a</th>\n",
              "      <th>mean_d_20_a</th>\n",
              "      <th>mean_d_21_a</th>\n",
              "      <th>mean_d_22_a</th>\n",
              "      <th>mean_d_23_a</th>\n",
              "      <th>mean_d_24_a</th>\n",
              "      <th>mean_d_25_a</th>\n",
              "      <th>mean_d_26_a</th>\n",
              "      <th>mean_d_27_a</th>\n",
              "      <th>mean_d_28_a</th>\n",
              "      <th>mean_d_29_a</th>\n",
              "      <th>...</th>\n",
              "      <th>fft_711_b</th>\n",
              "      <th>fft_712_b</th>\n",
              "      <th>fft_713_b</th>\n",
              "      <th>fft_714_b</th>\n",
              "      <th>fft_715_b</th>\n",
              "      <th>fft_716_b</th>\n",
              "      <th>fft_717_b</th>\n",
              "      <th>fft_718_b</th>\n",
              "      <th>fft_719_b</th>\n",
              "      <th>fft_720_b</th>\n",
              "      <th>fft_721_b</th>\n",
              "      <th>fft_722_b</th>\n",
              "      <th>fft_723_b</th>\n",
              "      <th>fft_724_b</th>\n",
              "      <th>fft_725_b</th>\n",
              "      <th>fft_726_b</th>\n",
              "      <th>fft_727_b</th>\n",
              "      <th>fft_728_b</th>\n",
              "      <th>fft_729_b</th>\n",
              "      <th>fft_730_b</th>\n",
              "      <th>fft_731_b</th>\n",
              "      <th>fft_732_b</th>\n",
              "      <th>fft_733_b</th>\n",
              "      <th>fft_734_b</th>\n",
              "      <th>fft_735_b</th>\n",
              "      <th>fft_736_b</th>\n",
              "      <th>fft_737_b</th>\n",
              "      <th>fft_738_b</th>\n",
              "      <th>fft_739_b</th>\n",
              "      <th>fft_740_b</th>\n",
              "      <th>fft_741_b</th>\n",
              "      <th>fft_742_b</th>\n",
              "      <th>fft_743_b</th>\n",
              "      <th>fft_744_b</th>\n",
              "      <th>fft_745_b</th>\n",
              "      <th>fft_746_b</th>\n",
              "      <th>fft_747_b</th>\n",
              "      <th>fft_748_b</th>\n",
              "      <th>fft_749_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.62</td>\n",
              "      <td>30.3</td>\n",
              "      <td>-356.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>26.3</td>\n",
              "      <td>1.070</td>\n",
              "      <td>0.411</td>\n",
              "      <td>-15.70</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.15</td>\n",
              "      <td>29.5</td>\n",
              "      <td>-353.0</td>\n",
              "      <td>14.40</td>\n",
              "      <td>21.5</td>\n",
              "      <td>5.98</td>\n",
              "      <td>30.7</td>\n",
              "      <td>-343.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>27.9</td>\n",
              "      <td>3.17</td>\n",
              "      <td>32.2</td>\n",
              "      <td>-368.0</td>\n",
              "      <td>15.9</td>\n",
              "      <td>36.4</td>\n",
              "      <td>7.08</td>\n",
              "      <td>28.8</td>\n",
              "      <td>-359.0</td>\n",
              "      <td>17.3</td>\n",
              "      <td>19.6</td>\n",
              "      <td>-3.8300</td>\n",
              "      <td>-1.230</td>\n",
              "      <td>-10.80000</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>-6.41</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-2.780</td>\n",
              "      <td>14.60</td>\n",
              "      <td>-1.540</td>\n",
              "      <td>-14.900000</td>\n",
              "      <td>...</td>\n",
              "      <td>30.90</td>\n",
              "      <td>-24.4</td>\n",
              "      <td>-24.4</td>\n",
              "      <td>30.90</td>\n",
              "      <td>-442.0</td>\n",
              "      <td>388.0</td>\n",
              "      <td>-197.0</td>\n",
              "      <td>-197.0</td>\n",
              "      <td>388.0</td>\n",
              "      <td>-564.0</td>\n",
              "      <td>500.00</td>\n",
              "      <td>-245.00</td>\n",
              "      <td>-245.00</td>\n",
              "      <td>500.00</td>\n",
              "      <td>-88.8</td>\n",
              "      <td>214.0</td>\n",
              "      <td>-88.8</td>\n",
              "      <td>-88.8</td>\n",
              "      <td>214.0</td>\n",
              "      <td>-606.0</td>\n",
              "      <td>509.000</td>\n",
              "      <td>-261.0</td>\n",
              "      <td>-261.0</td>\n",
              "      <td>509.000</td>\n",
              "      <td>-399.0</td>\n",
              "      <td>374.0</td>\n",
              "      <td>-185.00</td>\n",
              "      <td>-185.00</td>\n",
              "      <td>374.0</td>\n",
              "      <td>74.3</td>\n",
              "      <td>23.5</td>\n",
              "      <td>20.3</td>\n",
              "      <td>20.3</td>\n",
              "      <td>23.5</td>\n",
              "      <td>-215.0</td>\n",
              "      <td>280.00</td>\n",
              "      <td>-162.00</td>\n",
              "      <td>-162.00</td>\n",
              "      <td>280.00</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.80</td>\n",
              "      <td>33.1</td>\n",
              "      <td>32.0</td>\n",
              "      <td>25.8</td>\n",
              "      <td>22.8</td>\n",
              "      <td>6.550</td>\n",
              "      <td>1.680</td>\n",
              "      <td>2.88</td>\n",
              "      <td>3.83</td>\n",
              "      <td>-4.82</td>\n",
              "      <td>25.60</td>\n",
              "      <td>32.8</td>\n",
              "      <td>29.6</td>\n",
              "      <td>21.50</td>\n",
              "      <td>17.4</td>\n",
              "      <td>25.50</td>\n",
              "      <td>31.7</td>\n",
              "      <td>31.5</td>\n",
              "      <td>26.2</td>\n",
              "      <td>32.9</td>\n",
              "      <td>31.80</td>\n",
              "      <td>33.1</td>\n",
              "      <td>33.2</td>\n",
              "      <td>28.5</td>\n",
              "      <td>26.8</td>\n",
              "      <td>32.40</td>\n",
              "      <td>34.7</td>\n",
              "      <td>33.8</td>\n",
              "      <td>27.0</td>\n",
              "      <td>14.2</td>\n",
              "      <td>0.0342</td>\n",
              "      <td>1.100</td>\n",
              "      <td>-1.87000</td>\n",
              "      <td>-4.690</td>\n",
              "      <td>-15.40</td>\n",
              "      <td>-6.22</td>\n",
              "      <td>-0.328</td>\n",
              "      <td>-3.53</td>\n",
              "      <td>-6.980</td>\n",
              "      <td>-9.370000</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.99</td>\n",
              "      <td>-19.1</td>\n",
              "      <td>-19.1</td>\n",
              "      <td>-5.99</td>\n",
              "      <td>163.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-10.7</td>\n",
              "      <td>-10.7</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>-3.93</td>\n",
              "      <td>17.90</td>\n",
              "      <td>17.90</td>\n",
              "      <td>-3.93</td>\n",
              "      <td>112.0</td>\n",
              "      <td>-13.9</td>\n",
              "      <td>25.5</td>\n",
              "      <td>25.5</td>\n",
              "      <td>-13.9</td>\n",
              "      <td>225.0</td>\n",
              "      <td>-0.968</td>\n",
              "      <td>-27.7</td>\n",
              "      <td>-27.7</td>\n",
              "      <td>-0.968</td>\n",
              "      <td>97.4</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>40.70</td>\n",
              "      <td>40.70</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>-23.3</td>\n",
              "      <td>-21.8</td>\n",
              "      <td>-21.8</td>\n",
              "      <td>-23.3</td>\n",
              "      <td>182.0</td>\n",
              "      <td>2.57</td>\n",
              "      <td>-31.60</td>\n",
              "      <td>-31.60</td>\n",
              "      <td>2.57</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.90</td>\n",
              "      <td>29.4</td>\n",
              "      <td>-416.0</td>\n",
              "      <td>16.7</td>\n",
              "      <td>23.7</td>\n",
              "      <td>79.900</td>\n",
              "      <td>3.360</td>\n",
              "      <td>90.20</td>\n",
              "      <td>89.90</td>\n",
              "      <td>2.03</td>\n",
              "      <td>7.75</td>\n",
              "      <td>30.1</td>\n",
              "      <td>-441.0</td>\n",
              "      <td>9.89</td>\n",
              "      <td>25.3</td>\n",
              "      <td>-68.90</td>\n",
              "      <td>25.3</td>\n",
              "      <td>-481.0</td>\n",
              "      <td>-65.4</td>\n",
              "      <td>20.0</td>\n",
              "      <td>79.80</td>\n",
              "      <td>31.0</td>\n",
              "      <td>-408.0</td>\n",
              "      <td>91.9</td>\n",
              "      <td>29.5</td>\n",
              "      <td>18.80</td>\n",
              "      <td>31.1</td>\n",
              "      <td>-335.0</td>\n",
              "      <td>32.2</td>\n",
              "      <td>19.9</td>\n",
              "      <td>76.6000</td>\n",
              "      <td>4.850</td>\n",
              "      <td>39.90000</td>\n",
              "      <td>75.300</td>\n",
              "      <td>5.27</td>\n",
              "      <td>-72.00</td>\n",
              "      <td>-0.843</td>\n",
              "      <td>-33.00</td>\n",
              "      <td>-82.000</td>\n",
              "      <td>-4.200000</td>\n",
              "      <td>...</td>\n",
              "      <td>585.00</td>\n",
              "      <td>-285.0</td>\n",
              "      <td>-285.0</td>\n",
              "      <td>585.00</td>\n",
              "      <td>-94.3</td>\n",
              "      <td>183.0</td>\n",
              "      <td>-110.0</td>\n",
              "      <td>-110.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>-133.0</td>\n",
              "      <td>200.00</td>\n",
              "      <td>-72.00</td>\n",
              "      <td>-72.00</td>\n",
              "      <td>200.00</td>\n",
              "      <td>-356.0</td>\n",
              "      <td>295.0</td>\n",
              "      <td>-158.0</td>\n",
              "      <td>-158.0</td>\n",
              "      <td>295.0</td>\n",
              "      <td>-251.0</td>\n",
              "      <td>255.000</td>\n",
              "      <td>-95.7</td>\n",
              "      <td>-95.7</td>\n",
              "      <td>255.000</td>\n",
              "      <td>-177.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>-48.80</td>\n",
              "      <td>-48.80</td>\n",
              "      <td>159.0</td>\n",
              "      <td>-534.0</td>\n",
              "      <td>462.0</td>\n",
              "      <td>-233.0</td>\n",
              "      <td>-233.0</td>\n",
              "      <td>462.0</td>\n",
              "      <td>-267.0</td>\n",
              "      <td>281.00</td>\n",
              "      <td>-148.00</td>\n",
              "      <td>-148.00</td>\n",
              "      <td>281.00</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.90</td>\n",
              "      <td>31.6</td>\n",
              "      <td>-143.0</td>\n",
              "      <td>19.8</td>\n",
              "      <td>24.3</td>\n",
              "      <td>-0.584</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>8.82</td>\n",
              "      <td>2.30</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>17.30</td>\n",
              "      <td>32.0</td>\n",
              "      <td>-148.0</td>\n",
              "      <td>20.40</td>\n",
              "      <td>22.8</td>\n",
              "      <td>13.20</td>\n",
              "      <td>31.5</td>\n",
              "      <td>-147.0</td>\n",
              "      <td>16.9</td>\n",
              "      <td>27.7</td>\n",
              "      <td>15.70</td>\n",
              "      <td>30.7</td>\n",
              "      <td>-142.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>22.8</td>\n",
              "      <td>13.60</td>\n",
              "      <td>32.2</td>\n",
              "      <td>-135.0</td>\n",
              "      <td>21.2</td>\n",
              "      <td>23.8</td>\n",
              "      <td>4.1500</td>\n",
              "      <td>0.556</td>\n",
              "      <td>-0.35400</td>\n",
              "      <td>3.460</td>\n",
              "      <td>-4.96</td>\n",
              "      <td>1.63</td>\n",
              "      <td>1.330</td>\n",
              "      <td>-5.83</td>\n",
              "      <td>-0.298</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>...</td>\n",
              "      <td>249.00</td>\n",
              "      <td>-146.0</td>\n",
              "      <td>-146.0</td>\n",
              "      <td>249.00</td>\n",
              "      <td>359.0</td>\n",
              "      <td>-146.0</td>\n",
              "      <td>13.7</td>\n",
              "      <td>13.7</td>\n",
              "      <td>-146.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>-7.64</td>\n",
              "      <td>-7.17</td>\n",
              "      <td>-7.17</td>\n",
              "      <td>-7.64</td>\n",
              "      <td>-296.0</td>\n",
              "      <td>316.0</td>\n",
              "      <td>-218.0</td>\n",
              "      <td>-218.0</td>\n",
              "      <td>316.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>-113.000</td>\n",
              "      <td>38.4</td>\n",
              "      <td>38.4</td>\n",
              "      <td>-113.000</td>\n",
              "      <td>245.0</td>\n",
              "      <td>-61.9</td>\n",
              "      <td>-5.08</td>\n",
              "      <td>-5.08</td>\n",
              "      <td>-61.9</td>\n",
              "      <td>-183.0</td>\n",
              "      <td>299.0</td>\n",
              "      <td>-243.0</td>\n",
              "      <td>-243.0</td>\n",
              "      <td>299.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>-12.40</td>\n",
              "      <td>9.53</td>\n",
              "      <td>9.53</td>\n",
              "      <td>-12.40</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.30</td>\n",
              "      <td>31.3</td>\n",
              "      <td>45.2</td>\n",
              "      <td>27.3</td>\n",
              "      <td>24.5</td>\n",
              "      <td>34.800</td>\n",
              "      <td>-5.790</td>\n",
              "      <td>3.06</td>\n",
              "      <td>41.40</td>\n",
              "      <td>5.52</td>\n",
              "      <td>26.10</td>\n",
              "      <td>34.3</td>\n",
              "      <td>43.7</td>\n",
              "      <td>23.70</td>\n",
              "      <td>20.6</td>\n",
              "      <td>-3.87</td>\n",
              "      <td>34.1</td>\n",
              "      <td>43.7</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>22.9</td>\n",
              "      <td>59.40</td>\n",
              "      <td>26.7</td>\n",
              "      <td>60.3</td>\n",
              "      <td>64.7</td>\n",
              "      <td>26.9</td>\n",
              "      <td>32.30</td>\n",
              "      <td>30.1</td>\n",
              "      <td>33.6</td>\n",
              "      <td>31.6</td>\n",
              "      <td>27.7</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-0.00677</td>\n",
              "      <td>33.700</td>\n",
              "      <td>-2.36</td>\n",
              "      <td>-33.30</td>\n",
              "      <td>7.640</td>\n",
              "      <td>-16.60</td>\n",
              "      <td>-41.100</td>\n",
              "      <td>-6.290000</td>\n",
              "      <td>...</td>\n",
              "      <td>18.30</td>\n",
              "      <td>-11.9</td>\n",
              "      <td>-11.9</td>\n",
              "      <td>18.30</td>\n",
              "      <td>159.0</td>\n",
              "      <td>-18.2</td>\n",
              "      <td>-25.6</td>\n",
              "      <td>-25.6</td>\n",
              "      <td>-18.2</td>\n",
              "      <td>194.0</td>\n",
              "      <td>32.70</td>\n",
              "      <td>-54.50</td>\n",
              "      <td>-54.50</td>\n",
              "      <td>32.70</td>\n",
              "      <td>154.0</td>\n",
              "      <td>26.4</td>\n",
              "      <td>-56.4</td>\n",
              "      <td>-56.4</td>\n",
              "      <td>26.4</td>\n",
              "      <td>252.0</td>\n",
              "      <td>9.940</td>\n",
              "      <td>-37.4</td>\n",
              "      <td>-37.4</td>\n",
              "      <td>9.940</td>\n",
              "      <td>172.0</td>\n",
              "      <td>15.9</td>\n",
              "      <td>-7.25</td>\n",
              "      <td>-7.25</td>\n",
              "      <td>15.9</td>\n",
              "      <td>114.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>38.1</td>\n",
              "      <td>38.1</td>\n",
              "      <td>12.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>-17.60</td>\n",
              "      <td>23.90</td>\n",
              "      <td>23.90</td>\n",
              "      <td>-17.60</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2549 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   # mean_0_a  mean_1_a  mean_2_a  ...  fft_748_b  fft_749_b     label\n",
              "0        4.62      30.3    -356.0  ...    -162.00     280.00  NEGATIVE\n",
              "1       28.80      33.1      32.0  ...     -31.60       2.57   NEUTRAL\n",
              "2        8.90      29.4    -416.0  ...    -148.00     281.00  POSITIVE\n",
              "3       14.90      31.6    -143.0  ...       9.53     -12.40  POSITIVE\n",
              "4       28.30      31.3      45.2  ...      23.90     -17.60   NEUTRAL\n",
              "\n",
              "[5 rows x 2549 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3bPJri9ubAe"
      },
      "source": [
        "measurements.loc[measurements[\"label\"] == 'NEUTRAL', \"label\"] = 0\n",
        "measurements.loc[measurements[\"label\"] == 'POSITIVE', \"label\"] = 1\n",
        "measurements.loc[measurements[\"label\"] == 'NEGATIVE', \"label\"] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jwRo16juhAQ"
      },
      "source": [
        "x=measurements.iloc[:,:-1]\n",
        "y=measurements.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvnQys2XurKj",
        "outputId": "507e3bc0-0872-4c1e-a124-6d0537dd7d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFBWX9Wl0JBW"
      },
      "source": [
        "x = np.array(x[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTmxBpeO2hJz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1,random_state = 120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b_902xJ237E"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.fit_transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDg68gDj26KC"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trRVcr4V28-i",
        "outputId": "2b5ca789-2b8b-4f20-c1d2-a2567640934e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1918, 2548, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAdEgBzA3Efm",
        "outputId": "04b6ffbb-d333-4038-b9c8-e8dcf7d3b8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 3\n",
        "epochs = 100\n",
        "input_shape=(x_train.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c5881a90eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu-af3513NBQ",
        "outputId": "b6de8c8f-3d44-4ea2-d6a8-d71206a0af09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2548, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgniVoDG3VeC"
      },
      "source": [
        "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK89vuuj3Zk0",
        "outputId": "39d12fed-9189-4e67-f0b0-897665fb3fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "model = Sequential()\n",
        "intput_shape=(x_train.shape[1], 1)\n",
        "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "#model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_39 (Conv1D)           (None, 2548, 128)         512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 2548, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_39 (MaxPooling (None, 1274, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_40 (Conv1D)           (None, 1274, 128)         49280     \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 1274, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_40 (MaxPooling (None, 637, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 81536)             0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 64)                5218368   \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 5,271,843\n",
            "Trainable params: 5,271,331\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq776yjK3k0H"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbU-k1t2rbET",
        "outputId": "8360a3ae-c982-45e6-d685-062f24dfe201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.8857 - accuracy: 0.6366\n",
            "Epoch 2/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.6165 - accuracy: 0.7899\n",
            "Epoch 3/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.4792 - accuracy: 0.8608\n",
            "Epoch 4/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.4011 - accuracy: 0.8790\n",
            "Epoch 5/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.3488 - accuracy: 0.8942\n",
            "Epoch 6/100\n",
            "1918/1918 [==============================] - 41s 21ms/step - loss: 0.3162 - accuracy: 0.8973\n",
            "Epoch 7/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.3045 - accuracy: 0.9062\n",
            "Epoch 8/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.2847 - accuracy: 0.8989\n",
            "Epoch 9/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.2942 - accuracy: 0.9015\n",
            "Epoch 10/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2590 - accuracy: 0.9145\n",
            "Epoch 11/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2488 - accuracy: 0.9208\n",
            "Epoch 12/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2516 - accuracy: 0.9140\n",
            "Epoch 13/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2325 - accuracy: 0.9239\n",
            "Epoch 14/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2318 - accuracy: 0.9244\n",
            "Epoch 15/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.2349 - accuracy: 0.9161\n",
            "Epoch 16/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.2298 - accuracy: 0.9192\n",
            "Epoch 17/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.2396 - accuracy: 0.9181\n",
            "Epoch 18/100\n",
            "1918/1918 [==============================] - 39s 21ms/step - loss: 0.2248 - accuracy: 0.9281\n",
            "Epoch 19/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2336 - accuracy: 0.9312\n",
            "Epoch 20/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.2145 - accuracy: 0.9338\n",
            "Epoch 21/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1963 - accuracy: 0.9333\n",
            "Epoch 22/100\n",
            "1918/1918 [==============================] - 41s 21ms/step - loss: 0.2103 - accuracy: 0.9343\n",
            "Epoch 23/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1949 - accuracy: 0.9359\n",
            "Epoch 24/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1826 - accuracy: 0.9437\n",
            "Epoch 25/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1764 - accuracy: 0.9359\n",
            "Epoch 26/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1863 - accuracy: 0.9343\n",
            "Epoch 27/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.1955 - accuracy: 0.9369\n",
            "Epoch 28/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1676 - accuracy: 0.9442\n",
            "Epoch 29/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.1614 - accuracy: 0.9442\n",
            "Epoch 30/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1598 - accuracy: 0.9453\n",
            "Epoch 31/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1633 - accuracy: 0.9442\n",
            "Epoch 32/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1473 - accuracy: 0.9484\n",
            "Epoch 33/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1701 - accuracy: 0.9442\n",
            "Epoch 34/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1502 - accuracy: 0.9515\n",
            "Epoch 35/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1527 - accuracy: 0.9499\n",
            "Epoch 36/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1475 - accuracy: 0.9473\n",
            "Epoch 37/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1401 - accuracy: 0.9515\n",
            "Epoch 38/100\n",
            "1918/1918 [==============================] - 40s 21ms/step - loss: 0.1377 - accuracy: 0.9578\n",
            "Epoch 39/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1325 - accuracy: 0.9572\n",
            "Epoch 40/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1457 - accuracy: 0.9479\n",
            "Epoch 41/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1338 - accuracy: 0.9562\n",
            "Epoch 42/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1390 - accuracy: 0.9499\n",
            "Epoch 43/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1524 - accuracy: 0.9505\n",
            "Epoch 44/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1467 - accuracy: 0.9479\n",
            "Epoch 45/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1601 - accuracy: 0.9437\n",
            "Epoch 46/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1487 - accuracy: 0.9432\n",
            "Epoch 47/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1460 - accuracy: 0.9484\n",
            "Epoch 48/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1483 - accuracy: 0.9526\n",
            "Epoch 49/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1506 - accuracy: 0.9489\n",
            "Epoch 50/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1475 - accuracy: 0.9442\n",
            "Epoch 51/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1260 - accuracy: 0.9578\n",
            "Epoch 52/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1409 - accuracy: 0.9505\n",
            "Epoch 53/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1453 - accuracy: 0.9489\n",
            "Epoch 54/100\n",
            "1918/1918 [==============================] - 40s 21ms/step - loss: 0.1390 - accuracy: 0.9489\n",
            "Epoch 55/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1312 - accuracy: 0.9557\n",
            "Epoch 56/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1297 - accuracy: 0.9546\n",
            "Epoch 57/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1253 - accuracy: 0.9567\n",
            "Epoch 58/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1002 - accuracy: 0.9614\n",
            "Epoch 59/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1119 - accuracy: 0.9578\n",
            "Epoch 60/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1050 - accuracy: 0.9630\n",
            "Epoch 61/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0937 - accuracy: 0.9666\n",
            "Epoch 62/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1143 - accuracy: 0.9593\n",
            "Epoch 63/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1028 - accuracy: 0.9604\n",
            "Epoch 64/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0939 - accuracy: 0.9677\n",
            "Epoch 65/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1178 - accuracy: 0.9531\n",
            "Epoch 66/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0965 - accuracy: 0.9640\n",
            "Epoch 67/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1189 - accuracy: 0.9583\n",
            "Epoch 68/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1172 - accuracy: 0.9536\n",
            "Epoch 69/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.1045 - accuracy: 0.9614\n",
            "Epoch 70/100\n",
            "1918/1918 [==============================] - 39s 21ms/step - loss: 0.1098 - accuracy: 0.9614\n",
            "Epoch 71/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0897 - accuracy: 0.9703\n",
            "Epoch 72/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0749 - accuracy: 0.9734\n",
            "Epoch 73/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0874 - accuracy: 0.9672\n",
            "Epoch 74/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0838 - accuracy: 0.9713\n",
            "Epoch 75/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0725 - accuracy: 0.9776\n",
            "Epoch 76/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0828 - accuracy: 0.9718\n",
            "Epoch 77/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0862 - accuracy: 0.9713\n",
            "Epoch 78/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0743 - accuracy: 0.9745\n",
            "Epoch 79/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0740 - accuracy: 0.9739\n",
            "Epoch 80/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0731 - accuracy: 0.9760\n",
            "Epoch 81/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0684 - accuracy: 0.9739\n",
            "Epoch 82/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0695 - accuracy: 0.9797\n",
            "Epoch 83/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0766 - accuracy: 0.9739\n",
            "Epoch 84/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0719 - accuracy: 0.9760\n",
            "Epoch 85/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.0756 - accuracy: 0.9745\n",
            "Epoch 86/100\n",
            "1918/1918 [==============================] - 39s 20ms/step - loss: 0.0893 - accuracy: 0.9713\n",
            "Epoch 87/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0780 - accuracy: 0.9724\n",
            "Epoch 88/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0961 - accuracy: 0.9682\n",
            "Epoch 89/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1139 - accuracy: 0.9708\n",
            "Epoch 90/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0853 - accuracy: 0.9698\n",
            "Epoch 91/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0767 - accuracy: 0.9718\n",
            "Epoch 92/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0849 - accuracy: 0.9687\n",
            "Epoch 93/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0734 - accuracy: 0.9739\n",
            "Epoch 94/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0809 - accuracy: 0.9718\n",
            "Epoch 95/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0846 - accuracy: 0.9739\n",
            "Epoch 96/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0465 - accuracy: 0.9854\n",
            "Epoch 97/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0809 - accuracy: 0.9745\n",
            "Epoch 98/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0686 - accuracy: 0.9791\n",
            "Epoch 99/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.0875 - accuracy: 0.9703\n",
            "Epoch 100/100\n",
            "1918/1918 [==============================] - 38s 20ms/step - loss: 0.1013 - accuracy: 0.9651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1613ed6ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGNkik2-3qru",
        "outputId": "31fa66c8-c1ff-4e6c-d091-74be87c59fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "214/214 [==============================] - 1s 6ms/step\n",
            "Test loss: 0.2074207203271233\n",
            "Test accuracy: 0.9439252614974976\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}